{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dataset.ipynb","provenance":[],"authorship_tag":"ABX9TyMG25whKk6PSi+doSVsz382"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"oAECaU_4TfDp"},"source":["import torch\n","import cv2\n","import pandas as pd\n","import numpy as np\n","import config\n","import utils\n","from torch.utils.data import Dataset, DataLoader\n","\n","def train_test_split(csv_path, split):\n","    df_data = pd.read_csv(csv_path)\n","    len_data = len(df_data)\n","    # calculate the validation data sample length\n","    valid_split = int(len_data * split)\n","    # calculate the training data samples length\n","    train_split = int(len_data - valid_split)\n","    training_samples = df_data.iloc[:train_split][:]\n","    valid_samples = df_data.iloc[-valid_split:][:]\n","    return training_samples, valid_samples\n","\n","class FaceKeypointDataset(Dataset):\n","    def __init__(self, samples, path):\n","        self.data = samples\n","        self.path = path\n","        self.resize = 224\n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, index):\n","        image = cv2.imread(f\"{self.path}/{self.data.iloc[index][0]}\")\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        orig_h, orig_w, channel = image.shape\n","        # resize the image into `resize` defined above\n","        image = cv2.resize(image, (self.resize, self.resize))\n","        # again reshape to add grayscale channel format\n","        image = image / 255.0\n","        # transpose for getting the channel size to index 0\n","        image = np.transpose(image, (2, 0, 1))\n","        # get the keypoints\n","        keypoints = self.data.iloc[index][1:]\n","        keypoints = np.array(keypoints, dtype='float32')\n","        # reshape the keypoints\n","        keypoints = keypoints.reshape(-1, 2)\n","        # rescale keypoints according to image resize\n","        keypoints = keypoints * [self.resize / orig_w, self.resize / orig_h]\n","        return {\n","            'image': torch.tensor(image, dtype=torch.float),\n","            'keypoints': torch.tensor(keypoints, dtype=torch.float),\n","        }\n","\n","\n","# get the training and validation data samples\n","training_samples, valid_samples = train_test_split(f\"{config.ROOT_PATH}/training_frames_keypoints.csv\",\n","                                                   config.TEST_SPLIT)\n","# initialize the dataset - `FaceKeypointDataset()`\n","train_data = FaceKeypointDataset(training_samples, \n","                                 f\"{config.ROOT_PATH}/training\")\n","valid_data = FaceKeypointDataset(valid_samples, \n","                                 f\"{config.ROOT_PATH}/training\")\n","# prepare data loaders\n","train_loader = DataLoader(train_data, \n","                          batch_size=config.BATCH_SIZE, \n","                          shuffle=True)\n","valid_loader = DataLoader(valid_data, \n","                          batch_size=config.BATCH_SIZE, \n","                          shuffle=False)\n","print(f\"Training sample instances: {len(train_data)}\")\n","print(f\"Validation sample instances: {len(valid_data)}\")\n","\n","# whether to show dataset keypoint plots\n","if config.SHOW_DATASET_PLOT:\n","    utils.dataset_keypoints_plot(valid_data)\n"],"execution_count":null,"outputs":[]}]}